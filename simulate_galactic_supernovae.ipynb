{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from starccato_flow.nn.cvae import ConditionalVAE\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate galactic supernovae from simple galactic disk model\n",
    "\n",
    "# sky = Sky(ra=0.0, dec=0.0)\n",
    "# supernovae = sky.generate_galactic_supernovae(num_supernovae=10000)\n",
    "\n",
    "# # plot supernova in 3d plot, and 2d plot\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# ax1 = fig.add_subplot(121, projection='3d')\n",
    "# ax2 = fig.add_subplot(122)\n",
    "# x = supernovae[:, 0]\n",
    "# y = supernovae[:, 1]    \n",
    "# z = supernovae[:, 2]\n",
    "# ax1.scatter(x, y, z, s=0.001, alpha=0.1)\n",
    "# ax1.set_xlabel('X (kpc)')\n",
    "# ax1.set_ylabel('Y (kpc)')\n",
    "# ax1.set_zlabel('Z (kpc)')\n",
    "# ax1.set_title('3D Distribution of Galactic Supernovae')\n",
    "# ax2.scatter(x, y, s=1)\n",
    "# ax2.set_xlabel('X (kpc)')\n",
    "# ax2.set_ylabel('Y (kpc)')\n",
    "# ax2.set_title('2D Projection of Galactic Supernovae')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read file and plot\n",
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "# data = pd.read_csv('../../exploded_supernovae_t100_sf5.csv')\n",
    "\n",
    "# x = data['x_kpc'].values\n",
    "# y = data['y_kpc'].values    \n",
    "# z = data['z_kpc'].values\n",
    "\n",
    "# sun_location = [0.0, 8.178, 0.0208]\n",
    "\n",
    "# # 3D Distribution plot\n",
    "# fig1 = plt.figure(figsize=(16, 16), facecolor='black')\n",
    "# ax1 = fig1.add_subplot(111, projection='3d', facecolor='black')\n",
    "# ax1.scatter(x, y, z, s=0.001, alpha=1, c='lightblue')\n",
    "# ax1.scatter(sun_location[0], sun_location[1], sun_location[2], s=100, c='yellow', marker='*', label='Sun')\n",
    "# ax1.set_xlabel('X (kpc)', color='white')\n",
    "# ax1.set_ylabel('Y (kpc)', color='white')\n",
    "# ax1.set_zlabel('Z (kpc)', color='white')\n",
    "# ax1.tick_params(colors='white')\n",
    "# ax1.set_aspect('equal')\n",
    "# ax1.set_zticks([])\n",
    "# # Remove grey panes\n",
    "# ax1.xaxis.pane.set_facecolor('black')\n",
    "# ax1.yaxis.pane.set_facecolor('black')\n",
    "# ax1.zaxis.pane.set_facecolor('black')\n",
    "# ax1.xaxis.pane.set_edgecolor('white')\n",
    "# ax1.yaxis.pane.set_edgecolor('white')\n",
    "# ax1.zaxis.pane.set_edgecolor('white')\n",
    "# ax1.grid(color='gray', alpha=0.2)\n",
    "# # Remove z-axis grid lines\n",
    "# ax1.zaxis._axinfo['grid']['color'] = (0, 0, 0, 0)\n",
    "# # Center x-y plane at z=0 (middle of galaxy)\n",
    "# z_max = max(abs(z.min()), abs(z.max()))\n",
    "# ax1.set_zlim(-z_max, z_max)\n",
    "# plt.show()\n",
    "\n",
    "# # X-Y Projection plot\n",
    "# fig2 = plt.figure(figsize=(16, 16), facecolor='black')\n",
    "# ax2 = fig2.add_subplot(111, facecolor='black')\n",
    "# ax2.scatter(x, y, s=0.001, c='lightblue', alpha=1, marker='o')\n",
    "# ax2.scatter(sun_location[0], sun_location[1], s=100, c='yellow', marker='*', label='Sun')\n",
    "# ax2.set_xlabel('X (kpc)', color='white')\n",
    "# ax2.set_ylabel('Y (kpc)', color='white')\n",
    "# ax2.set_title('Simulated Galactic Core-Collapse Supernovae', color='white')\n",
    "# ax2.tick_params(colors='white')\n",
    "# ax2.spines['bottom'].set_color('white')\n",
    "# ax2.spines['left'].set_color('white')\n",
    "# ax2.spines['top'].set_color('white')\n",
    "# ax2.spines['right'].set_color('white')\n",
    "# ax2.set_aspect('equal')\n",
    "# plt.show()\n",
    "\n",
    "# # X-Z Projection plot\n",
    "# fig3 = plt.figure(figsize=(16, 16), facecolor='black')\n",
    "# ax3 = fig3.add_subplot(111, facecolor='black')\n",
    "# ax3.scatter(x, z, s=0.001, c='lightblue', alpha=1)\n",
    "# ax3.scatter(sun_location[0], sun_location[2], s=100, c='yellow', marker='*', label='Sun')\n",
    "# ax3.set_xlabel('X (kpc)', color='white')\n",
    "# ax3.set_ylabel('Z (kpc)', color='white')\n",
    "# ax3.tick_params(colors='white')\n",
    "# ax3.spines['bottom'].set_color('white')\n",
    "# ax3.spines['left'].set_color('white')\n",
    "# ax3.spines['top'].set_color('white')\n",
    "# ax3.spines['right'].set_color('white')\n",
    "# ax3.set_aspect('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sky projection from Sun's perspective\n",
    "# # Calculate positions relative to the Sun\n",
    "# x_rel = x - sun_location[0]\n",
    "# y_rel = y - sun_location[1]\n",
    "# z_rel = z - sun_location[2]\n",
    "\n",
    "# # Convert to spherical coordinates\n",
    "# # The galactic center is in the -y direction from the Sun\n",
    "# # We want galactic center at RA=0, Dec=0\n",
    "# # So we rotate the coordinates so that -y axis points to RA=0, Dec=0\n",
    "\n",
    "# # Calculate distance, RA, and Dec\n",
    "# distance = np.sqrt(x_rel**2 + y_rel**2 + z_rel**2)\n",
    "\n",
    "# # Standard astronomical convention: \n",
    "# # RA=0, Dec=0 points toward -y direction (galactic center)\n",
    "# # RA increases counterclockwise when viewed from north\n",
    "# ra = np.arctan2(x_rel, -y_rel) * 180 / np.pi  # Convert to degrees\n",
    "# dec = np.arcsin(z_rel / distance) * 180 / np.pi  # Convert to degrees\n",
    "\n",
    "# # Wrap RA to [0, 360) and convert to [-180, 180) for skymap\n",
    "# ra = ra % 360\n",
    "# ra_centered = np.where(ra > 180, ra - 360, ra)\n",
    "\n",
    "# # Convert to radians for projection\n",
    "# ra_rad = np.deg2rad(ra_centered)\n",
    "# dec_rad = np.deg2rad(dec)\n",
    "\n",
    "# # Create sky projection plot with Mollweide projection\n",
    "# fig4 = plt.figure(figsize=(16, 10), facecolor='black')\n",
    "# ax4 = fig4.add_subplot(111, projection='mollweide', facecolor='black')\n",
    "\n",
    "# # Plot supernovae\n",
    "# ax4.scatter(ra_rad, dec_rad, s=0.1, c='lightblue', alpha=0.5)\n",
    "\n",
    "# # Mark galactic center\n",
    "# ax4.scatter(0, 0, s=200, c='red', marker='x', linewidths=2, label='Galactic Center')\n",
    "\n",
    "# ax4.set_xlabel('Right Ascension (degrees)', color='white', fontsize=14)\n",
    "# ax4.set_ylabel('Declination (degrees)', color='white', fontsize=14)\n",
    "# ax4.set_title('Sky Map of Supernovae from Sun\\'s Perspective (Mollweide Projection)', color='white', fontsize=16)\n",
    "# ax4.tick_params(colors='white')\n",
    "# ax4.grid(color='gray', alpha=0.3, linestyle='--')\n",
    "# ax4.legend(facecolor='black', edgecolor='white', labelcolor='white', loc='upper right')\n",
    "\n",
    "# # Set x-axis labels to degrees\n",
    "# ax4.set_xticklabels(['180Â°', '135Â°', '90Â°', '45Â°', '0Â°', '-45Â°', '-90Â°', '-135Â°', '-180Â°'], color='white')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Total supernovae plotted: {len(ra)}\")\n",
    "# print(f\"RA range: {ra.min():.2f}Â° to {ra.max():.2f}Â°\")\n",
    "# print(f\"Dec range: {dec.min():.2f}Â° to {dec.max():.2f}Â°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained CVAE model with weights\n",
    "import torch\n",
    "from starccato_flow.utils.defaults import DEVICE\n",
    "\n",
    "# Define model architecture (must match training parameters)\n",
    "z_dim = 8  # Latent dimension used during training\n",
    "hidden_dim = 256  # Hidden layer dimension used during training (not 1024!)\n",
    "y_length = 256  # Signal length\n",
    "param_dim = 4  # Number of parameters (beta, omega, A, Ye)\n",
    "\n",
    "# Instantiate the model\n",
    "cvae = ConditionalVAE(z_dim=z_dim, hidden_dim=hidden_dim, y_length=y_length, param_dim=param_dim)\n",
    "\n",
    "# Load trained weights\n",
    "weights_path = 'outdir/cvae_weights.pt'\n",
    "cvae.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
    "cvae = cvae.to(DEVICE)\n",
    "cvae.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"âœ“ Loaded CVAE model from {weights_path}\")\n",
    "print(f\"  Architecture: z_dim={z_dim}, hidden_dim={hidden_dim}, param_dim={param_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals using uniform prior of parameters\n",
    "num_samples = 10000\n",
    "\n",
    "# Parameter ranges: [beta, omega, A, Ye]\n",
    "min_param_values = np.array([-1, -1, -1, -1])\n",
    "max_param_values = np.array([1, 1, 1, 1])\n",
    "\n",
    "# beta = np.random.uniform(-1, 1, num_samples)\n",
    "\n",
    "# omega = a * beta + b + noise # need to find a and b from training data \n",
    "\n",
    "# Generate random parameters uniformly within ranges\n",
    "params_unif = np.random.uniform(\n",
    "    min_param_values, \n",
    "    max_param_values, \n",
    "    size=(num_samples, 4)\n",
    ")\n",
    "\n",
    "# Define 5 discrete physical values for A (in km)\n",
    "discrete_A_physical = np.array([300, 467, 634, 1268.0, 10000.0])  # Physical values in km\n",
    "print(f\"Using 5 discrete A values (physical): {discrete_A_physical} km\")\n",
    "\n",
    "# We need training dataset to get normalization bounds\n",
    "from starccato_flow.data.ccsn_data import CCSNData\n",
    "temp_dataset = CCSNData(noise=False, curriculum=False)\n",
    "\n",
    "# Normalize each discrete A value to [-1, 1] space\n",
    "# A is at index 2 in the parameter array\n",
    "A_min = 300\n",
    "A_max = 10000\n",
    "discrete_A_normalized = 2 * (discrete_A_physical - A_min) / (A_max - A_min) - 1\n",
    "\n",
    "print(f\"Normalized A values: {discrete_A_normalized}\")\n",
    "\n",
    "# Randomly assign one of the 5 normalized values to each sample\n",
    "params_unif[:, 2] = np.random.choice(discrete_A_normalized, size=num_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "param_df = pd.DataFrame(\n",
    "    params_unif,\n",
    "    columns=[\"beta1_IC_b\", \"omega_0(rad|s)\", \"A(km)\", \"Ye_c_b\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {num_samples} parameter sets\")\n",
    "print(f\"\\nParameter ranges:\")\n",
    "print(f\"  beta:  [{min_param_values[0]:.3f}, {max_param_values[0]:.3f}] (continuous)\")\n",
    "print(f\"  omega: [{min_param_values[1]:.1f}, {max_param_values[1]:.1f}] (continuous)\")\n",
    "print(f\"  A:     5 discrete values: {discrete_A_physical} km\")\n",
    "print(f\"  Ye:    [{min_param_values[3]:.3f}, {max_param_values[3]:.3f}] (continuous)\")\n",
    "print(f\"\\nFirst few parameter sets:\")\n",
    "print(param_df.head())\n",
    "print(f\"\\nA value distribution (normalized):\")\n",
    "unique, counts = np.unique(params_unif[:, 2], return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    # Convert back to physical for display\n",
    "    A_physical = (val + 1) / 2 * (A_max - A_min) + A_min\n",
    "    print(f\"  A = {A_physical:.0f} km (norm: {val:.2f}): {count} samples ({100*count/num_samples:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd242f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize parameters and generate signals from CVAE\n",
    "from starccato_flow.data.ccsn_data import CCSNData\n",
    "from starccato_flow.utils.defaults import TEN_KPC\n",
    "\n",
    "# We need to normalize parameters to [-1, 1] for CVAE\n",
    "# Use the same normalization as the training data\n",
    "print(\"Loading training dataset for normalization...\")\n",
    "training_dataset = CCSNData(noise=False, curriculum=False)\n",
    "\n",
    "# Normalize the generated parameters\n",
    "# params_norm = training_dataset.normalize_parameters(params_unif)\n",
    "params_norm = params_unif\n",
    "\n",
    "print(f\"\\nGenerating {num_samples} signals from CVAE...\")\n",
    "# Convert to tensor\n",
    "params_tensor = torch.tensor(params_norm, dtype=torch.float32).to(DEVICE)\n",
    "z_samples = torch.randn(num_samples, z_dim).to(DEVICE)\n",
    "\n",
    "# Generate signals\n",
    "with torch.no_grad():\n",
    "    generated_signals = cvae.decoder(z_samples, params_tensor).cpu().numpy()\n",
    "\n",
    "# Denormalize signals back to physical units\n",
    "signals_denorm = []\n",
    "for i in range(num_samples):\n",
    "    signal_denorm = training_dataset.denormalise_signals(generated_signals[i])\n",
    "    signals_denorm.append(signal_denorm.flatten())\n",
    "\n",
    "signals_array = np.array(signals_denorm).T  # Shape: (256, num_samples)\n",
    "\n",
    "print(f\"âœ“ Generated {num_samples} signals\")\n",
    "print(f\"  Signal shape: {signals_array.shape}\")\n",
    "print(f\"  Signal range: [{signals_array.min():.2e}, {signals_array.max():.2e}] (in cm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d821a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of generated signals\n",
    "from starccato_flow.plotting import plot_signal_distribution\n",
    "\n",
    "print(\"Plotting signal distribution...\")\n",
    "plot_signal_distribution(\n",
    "    signals=signals_array / TEN_KPC,  # Convert to 10kpc distance\n",
    "    generated=True,\n",
    "    background=\"white\",\n",
    "    font_family=\"sans-serif\",\n",
    "    font_name=\"Avenir\",\n",
    "    fname=\"plots/cvae_generated_signal_distribution.svg\"\n",
    ")\n",
    "print(\"âœ“ Saved plot to plots/cvae_generated_signal_distribution.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of individual signals\n",
    "from starccato_flow.plotting import plot_signal_grid\n",
    "\n",
    "# Select 16 random signals to display in a 4x4 grid\n",
    "num_display = 16\n",
    "random_indices = np.random.choice(num_samples, size=num_display, replace=False)\n",
    "selected_signals = signals_array[:, random_indices].T  # Transpose to (num_signals, signal_length)\n",
    "\n",
    "print(f\"Plotting grid of {num_display} random signals...\")\n",
    "plot_signal_grid(\n",
    "    signals=selected_signals / TEN_KPC,  # Convert to 10kpc distance\n",
    "    noisy_signals=None,\n",
    "    num_cols=4,\n",
    "    num_rows=4,\n",
    "    fname=\"plots/cvae_generated_signal_grid.svg\",\n",
    "    max_value=1,\n",
    "    background=\"white\",\n",
    "    generated=True,\n",
    "    font_family=\"sans-serif\",\n",
    "    font_name=\"Avenir\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Saved plot to plots/cvae_generated_signal_grid.svg\")\n",
    "print(f\"\\nDisplayed signals have parameters:\")\n",
    "for i, idx in enumerate(random_indices[:4]):  # Show first 4\n",
    "    params = params_unif[idx]\n",
    "    print(f\"  Signal {i+1}: Î²={params[0]:.3f}, Ï‰={params[1]:.2f}, A={params[2]:.0f}, Ye={params[3]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ea63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter distributions (denormalized)\n",
    "# Denormalize parameters to physical units\n",
    "params_physical = training_dataset.denormalize_parameters(params_unif)\n",
    "\n",
    "# Create DataFrame with physical values\n",
    "param_df_physical = pd.DataFrame(\n",
    "    params_physical,\n",
    "    columns=[\"beta1_IC_b\", \"omega_0(rad|s)\", \"A(km)\", \"Ye_c_b\"]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), facecolor='white')\n",
    "fig.suptitle('Generated Parameter Distributions (Physical Units)', fontsize=16, fontweight='bold')\n",
    "\n",
    "param_names = [\"beta1_IC_b\", \"omega_0(rad|s)\", \"A(km)\", \"Ye_c_b\"]\n",
    "param_labels = [r'$\\beta_{IC,b}$', r'$\\omega_0$ (rad/s)', r'$A$ (km)', r'$Y_{e,c,b}$']\n",
    "\n",
    "for i, (name, label) in enumerate(zip(param_names, param_labels)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    values = param_df_physical[name].values\n",
    "    \n",
    "    # Plot histogram\n",
    "    ax.hist(values, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = values.mean()\n",
    "    std_val = values.std()\n",
    "    min_val = values.min()\n",
    "    max_val = values.max()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')\n",
    "    \n",
    "    ax.set_xlabel(label, fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title(f'{label} Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Add text box with stats\n",
    "    textstr = f'Î¼ = {mean_val:.3f}\\nÏƒ = {std_val:.3f}\\nmin = {min_val:.3f}\\nmax = {max_val:.3f}\\nN = {len(values)}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/parameter_distributions_physical.svg', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved parameter distribution plot to plots/parameter_distributions_physical.svg\")\n",
    "print(f\"\\nPhysical parameter ranges:\")\n",
    "for name, label in zip(param_names, param_labels):\n",
    "    vals = param_df_physical[name].values\n",
    "    print(f\"  {label}: [{vals.min():.3f}, {vals.max():.3f}], mean={vals.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a001b",
   "metadata": {},
   "source": [
    "## Using Custom Generated Data\n",
    "\n",
    "Now you can create a dataset from your generated signals and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save and reload your generated data\n",
    "# Save the generated signals and parameters\n",
    "np.save('outdir/generated_signals.npy', signals_array)\n",
    "np.save('outdir/generated_parameters.npy', params_physical)\n",
    "\n",
    "print(\"âœ“ Saved generated data to outdir/\")\n",
    "print(\"  - generated_signals.npy\")\n",
    "print(\"  - generated_parameters.npy\")\n",
    "\n",
    "# Later, you can reload and use them:\n",
    "# loaded_signals = np.load('outdir/generated_signals.npy')\n",
    "# loaded_params = np.load('outdir/generated_parameters.npy')\n",
    "# \n",
    "# reloaded_dataset = CCSNData(\n",
    "#     custom_data=(loaded_signals, loaded_params),\n",
    "#     noise=False,\n",
    "#     curriculum=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b897d4",
   "metadata": {},
   "source": [
    "### Using CVAE Validation Indices\n",
    "\n",
    "If you trained a CVAE and saved the validation indices, you can reuse them for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load REAL validation data from CVAE training (held-out test set)\n",
    "# These are the original ~160 real signals that were never used for CVAE training\n",
    "# We'll use them as the final test set for Flow Matching evaluation\n",
    "\n",
    "test_signals_real = np.load('outdir/cvae_val_signals.npy')  # Shape: (256, ~160)\n",
    "test_params_real = np.load('outdir/cvae_val_parameters.npy')  # Shape: (~160, 4)\n",
    "\n",
    "print(f\"âœ“ Loaded REAL validation data (CVAE held-out set)\")\n",
    "print(f\"  Test signals shape: {test_signals_real.shape}\")\n",
    "print(f\"  Test parameters shape: {test_params_real.shape}\")\n",
    "print(f\"  This is REAL data for final Flow Matching evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0673ba",
   "metadata": {},
   "source": [
    "## Train Flow Matching on Generated Data\n",
    "\n",
    "Now you can train a Flow Matching model on the CVAE-generated signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Flow Matching model on generated CVAE data\n",
    "from starccato_flow.training.trainer_flow_matching_new import FlowMatchingTrainerNew\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "# Create Flow Matching trainer with file paths\n",
    "# This will load:\n",
    "# - Training data: generated signals from CVAE (outdir/generated_*)\n",
    "# - Validation data: real CVAE validation set (outdir/cvae_val_*)\n",
    "flow_trainer = FlowMatchingTrainerNew(\n",
    "    num_epochs=num_epochs,\n",
    "    start_snr=100,\n",
    "    end_snr=10,\n",
    "    noise=True,\n",
    "    curriculum=True,\n",
    "    noise_realizations=1,\n",
    "    batch_size=64,\n",
    "    lr_flow=5e-4,\n",
    "    outdir=\"outdir/flow_matching\",\n",
    "    train_data_path=\"outdir/generated\",  # Loads generated_signals.npy and generated_parameters.npy\n",
    "    val_data_path=\"outdir/cvae_val\"  # Loads cvae_val_signals.npy and cvae_val_parameters.npy\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Flow Matching trainer initialized\")\n",
    "print(f\"  Training: {len(flow_trainer.training_dataset)} samples (generated from CVAE)\")\n",
    "print(f\"  Validation: {len(flow_trainer.validation_dataset)} samples (real CVAE held-out data)\")\n",
    "print(f\"\\nðŸ“Š Data Sources:\")\n",
    "print(f\"  â€¢ Training: ALL 20,000 generated signals\")\n",
    "print(f\"  â€¢ Validation: ~160 real signals held out from CVAE training\")\n",
    "print(f\"  â€¢ This tests: Can Flow Matching learn from synthetic data and generalize to real data?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50405b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "flow_trainer.train()\n",
    "\n",
    "# This will:\n",
    "# 1. Train Flow Matching on 20,000 generated signals from CVAE\n",
    "# 2. Validate on ~160 real signals (CVAE held-out set)\n",
    "# 3. Use curriculum learning (start with high SNR, gradually decrease to low SNR)\n",
    "# 4. Save checkpoints every 16 epochs to outdir/flow_matching/\n",
    "# 5. Track training and validation losses\n",
    "# 6. Test generalization: synthetic training data â†’ real validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66843323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, display results\n",
    "flow_trainer.display_results()\n",
    "\n",
    "# This will show:\n",
    "# - Training and validation loss curves\n",
    "# - Generated parameter distributions vs training data\n",
    "# - Corner plots showing parameter correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set validation dataset to first epoch before plotting corner plot\n",
    "flow_trainer.validation_dataset.set_epoch(0)\n",
    "flow_trainer.plot_corner(index=200, fname=\"plots/corner_plot.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PP plots for posterior calibration assessment\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Number of test samples for PP plot\n",
    "num_test_samples = 100\n",
    "num_posterior_samples = 1000\n",
    "\n",
    "print(f\"Generating PP plots with {num_test_samples} test samples...\")\n",
    "\n",
    "# Get device from the flow model\n",
    "device = next(flow_trainer.flow.parameters()).device\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get test data from validation set\n",
    "val_dataset = flow_trainer.validation_dataset\n",
    "test_indices = np.random.choice(len(val_dataset), size=min(num_test_samples, len(val_dataset)), replace=False)\n",
    "\n",
    "# Store percentile ranks for each parameter\n",
    "param_names = [\"beta1_IC_b\", \"omega_0(rad|s)\", \"A(km)\", \"Ye_c_b\"]\n",
    "param_labels = [r'$\\beta_{IC,b}$', r'$\\omega_0$ (rad/s)', r'$A$ (km)', r'$Y_{e,c,b}$']\n",
    "percentile_ranks = {name: [] for name in param_names}\n",
    "\n",
    "# Get parameter dimension from validation dataset\n",
    "param_dim = val_dataset.parameters.shape[1]\n",
    "print(f\"Parameter dimension: {param_dim}\")\n",
    "\n",
    "# For each test sample, compute posterior and find percentile rank of true value\n",
    "for i, idx in enumerate(test_indices):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processing test sample {i+1}/{num_test_samples}...\")\n",
    "    \n",
    "    # Get true signal and parameters (dataset may return 2 or 3 values)\n",
    "    dataset_output = val_dataset[idx]\n",
    "    if len(dataset_output) == 3:\n",
    "        signal, true_params, _ = dataset_output  # Unpack 3 values (signal, params, noise)\n",
    "    else:\n",
    "        signal, true_params = dataset_output  # Unpack 2 values (signal, params)\n",
    "    \n",
    "    signal = signal.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Sample from posterior using ODE integration\n",
    "    with torch.no_grad():\n",
    "        # Start from prior samples (noise in parameter space)\n",
    "        x_0 = torch.randn(num_posterior_samples, param_dim).to(device)\n",
    "        \n",
    "        # Integrate forward from t=0 to t=1 to get posterior samples\n",
    "        # Using multiple steps for more accurate integration\n",
    "        num_steps = 100\n",
    "        x_t = x_0\n",
    "        for step in range(num_steps):\n",
    "            t_start = torch.tensor(step / num_steps, device=device)\n",
    "            t_end = torch.tensor((step + 1) / num_steps, device=device)\n",
    "            x_t = flow_trainer.flow.step(x_t, t_start, t_end, signal.repeat(num_posterior_samples, 1))\n",
    "        \n",
    "        posterior_samples = x_t.cpu().numpy()\n",
    "    \n",
    "    # Denormalize true parameters and posterior samples\n",
    "    true_params_denorm = val_dataset.denormalize_parameters(true_params.cpu().numpy().reshape(1, -1))[0]\n",
    "    posterior_samples_denorm = val_dataset.denormalize_parameters(posterior_samples)\n",
    "    \n",
    "    # Calculate percentile rank of true value in posterior for each parameter\n",
    "    for j, name in enumerate(param_names):\n",
    "        true_val = true_params_denorm[j]\n",
    "        posterior_vals = posterior_samples_denorm[:, j]\n",
    "        \n",
    "        # Percentile rank: fraction of posterior samples less than true value\n",
    "        rank = np.mean(posterior_vals < true_val)\n",
    "        percentile_ranks[name].append(rank)\n",
    "\n",
    "# Create PP plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12), facecolor='white')\n",
    "fig.suptitle('PP Plots: Posterior Calibration Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (name, label) in enumerate(zip(param_names, param_labels)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    # Sort percentile ranks\n",
    "    ranks = np.sort(percentile_ranks[name])\n",
    "    \n",
    "    # Expected uniform distribution\n",
    "    expected = np.linspace(0, 1, len(ranks))\n",
    "    \n",
    "    # Plot PP curve\n",
    "    ax.plot(expected, ranks, 'o-', color='steelblue', markersize=3, label='Observed')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Ideal calibration')\n",
    "    \n",
    "    # Add confidence bands (approximate using binomial distribution)\n",
    "    n = len(ranks)\n",
    "    # 95% confidence interval for uniform distribution\n",
    "    lower = stats.binom.ppf(0.025, n, expected) / n\n",
    "    upper = stats.binom.ppf(0.975, n, expected) / n\n",
    "    ax.fill_between(expected, lower, upper, alpha=0.2, color='gray', label='95% CI')\n",
    "    \n",
    "    ax.set_xlabel('Expected cumulative probability', fontsize=12)\n",
    "    ax.set_ylabel('Observed cumulative probability', fontsize=12)\n",
    "    ax.set_title(f'{label}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Add KS test statistic\n",
    "    ks_stat = np.max(np.abs(ranks - expected))\n",
    "    ax.text(0.05, 0.95, f'KS stat: {ks_stat:.3f}', transform=ax.transAxes, \n",
    "            fontsize=10, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/pp_plot_posteriors.svg', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved PP plot to plots/pp_plot_posteriors.svg\")\n",
    "print(\"\\nCalibration summary:\")\n",
    "print(\"If points lie on the diagonal, posteriors are well-calibrated.\")\n",
    "print(\"Points above diagonal: posteriors are overconfident (too narrow).\")\n",
    "print(\"Points below diagonal: posteriors are underconfident (too wide).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ef9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from starccato_flow.data.toy_data import ToyData\n",
    "from starccato_flow.data.ccsn_data import CCSNData\n",
    "from starccato_flow.training.trainer import Trainer\n",
    "\n",
    "from starccato_flow.plotting.plotting import plot_reconstruction_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from starccato_flow.utils.defaults import DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473831fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from starccato_flow.data.ccsn_data import CCSNData\n",
    "from starccato_flow.utils.defaults import DEVICE\n",
    "\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms import CompositeTransform, ReversePermutation, MaskedAffineAutoregressiveTransform\n",
    "from nflows.flows import Flow\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_npe_with_vae(vae_trainer, num_epochs=20, batch_size=32, lr=1e-4, flow=None):\n",
    "    \"\"\"\n",
    "    Train a MaskedAutoregressiveFlow to estimate p(params | latent)\n",
    "    \"\"\"\n",
    "\n",
    "    vae = vae_trainer.vae\n",
    "    vae.eval()  # freeze VAE\n",
    "    latent_dim = 6\n",
    "    param_dim = 6  # your target parameter space\n",
    "\n",
    "    num_layers = 6\n",
    "    # create base dist and transforms in float32\n",
    "    base_dist = StandardNormal(shape=[param_dim])\n",
    "\n",
    "    # composite transform\n",
    "    transforms = []\n",
    "    for i in range(num_layers):\n",
    "        if i % 2 == 0:\n",
    "            transforms.append(ReversePermutation(features=param_dim))\n",
    "        transforms.append(\n",
    "            MaskedAffineAutoregressiveTransform(\n",
    "                features=param_dim,\n",
    "                hidden_features=128,\n",
    "                context_features=latent_dim\n",
    "            )\n",
    "        )\n",
    "\n",
    "    transform = CompositeTransform(transforms)\n",
    "\n",
    "    # create flow on CPU first, in float32\n",
    "    flow = Flow(transform, base_dist)\n",
    "\n",
    "    # move to device explicitly, MPS requires float32\n",
    "    flow = flow.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "    optimizer = optim.Adam(flow.parameters(), lr=lr)\n",
    "\n",
    "    ccsn_loader = DataLoader(\n",
    "        CCSNData(noise=True, curriculum=False),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (signal, noisy_signal, params) in enumerate(ccsn_loader):\n",
    "            signal = signal.to(DEVICE).float()\n",
    "            noisy_signal = noisy_signal.to(DEVICE).float()\n",
    "            params = params.to(DEVICE).float()\n",
    "\n",
    "            # Encode signal into latent space\n",
    "            with torch.no_grad():\n",
    "                _, mean, log_var = vae(noisy_signal)\n",
    "                z_latent = vae.reparameterization(mean, log_var)\n",
    "\n",
    "            # p(params | z)\n",
    "            params = params.view(params.size(0), -1) \n",
    "            z_latent = z_latent.view(z_latent.size(0), -1) \n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            log_prob = flow.log_prob(params, context=mean) # this conditions the flow on the latent variable z\n",
    "            loss = -log_prob.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Flow NLL: {total_loss / len(ccsn_loader):.4f}\")\n",
    "\n",
    "    return flow\n",
    "\n",
    "npe_flow = train_npe_with_vae(vae_trainer, num_epochs=50, batch_size=32, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f126a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vae_trainer.vae.eval()\n",
    "npe_flow.eval()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from starccato_flow.data.ccsn_data import CCSNData\n",
    "from starccato_flow.utils.defaults import DEVICE\n",
    "\n",
    "index = 1100\n",
    "\n",
    "signal = vae_trainer.training_dataset.__getitem__(index)[0]\n",
    "noisy_signal = vae_trainer.training_dataset.__getitem__(index)[1]\n",
    "params = vae_trainer.training_dataset.__getitem__(index)[2]\n",
    "\n",
    "# Ensure batch dimension [B, C, T]\n",
    "if noisy_signal.dim() == 2:\n",
    "    noisy_signal = noisy_signal.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noisy_signal = noisy_signal.to(DEVICE).float()\n",
    "    signal = signal.to(DEVICE).float()\n",
    "    _, mean, log_var = vae_trainer.vae(noisy_signal)\n",
    "    z = vae_trainer.vae.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "\n",
    "    # Use z as context\n",
    "    context = z.view(z.size(0), -1).to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "    # Sample from flow conditioned on z\n",
    "    num_draws = 1000\n",
    "    if context.size(0) != num_draws:\n",
    "        context = context.repeat(num_draws, 1)\n",
    "\n",
    "    samples = npe_flow.sample(num_samples=num_draws, context=context)\n",
    "    samples = samples.reshape(num_draws, -1)  # -> [num_draws, 6]\n",
    "\n",
    "    samples_cpu = samples.detach().cpu()\n",
    "    true_params = params.detach().cpu() if torch.is_tensor(params) else params\n",
    "    true_params = true_params.flatten()  # Flatten to [6] from [1, 6]\n",
    "    \n",
    "    print(\"True params:\", true_params)\n",
    "    print(\"Mean predicted:\", samples_cpu.mean(dim=0))\n",
    "    print(\"Std predicted:\", samples_cpu.std(dim=0))\n",
    "    \n",
    "    # Plot histogram of first parameter\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(samples_cpu[:, 0].numpy(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(true_params[0].item(), color='red', linestyle='--', linewidth=2, label=f'True value: {true_params[0].item():.3f}')\n",
    "    plt.axvline(samples_cpu[:, 0].mean().item(), color='green', linestyle='--', linewidth=2, label=f'Predicted mean: {samples_cpu[:, 0].mean().item():.3f}')\n",
    "    plt.xlabel('Parameter 1 Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Posterior Distribution of Parameter 1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
